{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $K$-Nearest Neighbors ($K$-NN)\n",
    "\n",
    "$k$-NN é um algoritmo de aprendizagem de máquina supervisionado que pode ser usado tanto para classificação quanto para regressão.\n",
    "\n",
    "É um algoritmo não pramétrico, aonde a estrutura do modelo será determinada pelo dataset utilizado. Este algoritmo utiliza uma forma de aprendizado conhecida como \"preguiçosa\" ( _lazy_), a qual não necessita de dados de treinamento para se gerar o modelo, deixando o treinamento mais rápido, mas por outro lado aumentando o tempo da etapa de teste. \n",
    "\n",
    "Outra desvantagem é que todos os dados obtidos no dataset devem ser armazenados para serem utilizados na fase de teste, resultando numa demanda maior por memória."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Como funciona o algoritmo KNN\n",
    "\n",
    "O $K$-NN possui um hiperparâmetro denominado $k$, o qual dá o nome ao modelo e determina a quantidade de vizinhos de cada amostra. Para problemas de classificação binária, geralmente utilízamos valores ímpares de $k$ a fim de evitar empates. \n",
    "\n",
    "Imagine, por exemplo, que gostariamos de classificar uma nova amostra entre classe 1 e classe 2 utilizando um valor de $k=1$. Tudo o que temos que fazer é encontrar o ponto mais próximo (podemos considerar diferentes distâncias, como Euclidiana, Hamming, Manhattan, etc.) dessa nova amostra e atribuir a ela o rótulo dessa amostra mais proxima. A imagem abaixo ilustra o exemplo:\n",
    "\n",
    "<img src=\"assets/knn_k1.png\" width=\"400\"/>\n",
    "\n",
    "\n",
    "Após identificar o ponto mais próximo e e seu rótulo (Ex.: Classe A), podemos predizer a classe novo ponto. \n",
    "Uma melhor abordagem seria, em vez de considerar apenas o vizinho mais próximo, fazer uma votação considerando a classe atribuída pelos $k$-vizinhos mais próximos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Passo a passo\n",
    "\n",
    "1. Calcular a distância da nova amostra para todos os pontos\n",
    "2. Encontrar os pontos/vizinhos mais próximos\n",
    "3. Votar a label para o ponto a ser previsto\n",
    "\n",
    "<img src=\"assets/stepByStep.png\" width=\"700\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predizendo a imagem de lesmas do mar com $K$-NN\n",
    "\n",
    "Para ilustrar o nosso algoritmo, vamos utilizar o dataset Abalone, que contém medidas de idade de um grande número de lesmas do mar.\n",
    "\n",
    "This dataset contains age measurements on a large number of abalones\n",
    "\n",
    "<img src=\"assets/abalone.jfif\" width=\"500\"/>\n",
    "\n",
    "\n",
    "A idade de um abalone pode ser encontrada cortando sua concha e contando o número de anéis dessa concha. O dataset apresenta esse número de anéias, assim como outras medidas físicas.\n",
    "\n",
    "O nosso objetivo aqui é conseguir estimar a idade do abalone baseando-se apenas nessas outras medidas físicas, permitindo conhecer a idade sem contar os aneis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## importando o dataset\n",
    "\n",
    "Considerando uma abordagem diferente, vamos utilizar o pacote Pandas para importar o dataset do repositório da [UCI](https://archive.ics.uci.edu/ml/datasets.php):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "url = (\n",
    "    \"https://archive.ics.uci.edu/ml/machine-learning-databases\"\n",
    "    \"/abalone/abalone.data\"\n",
    ")\n",
    "# com o pandas, conseguimos ler um arquivo CSV diretamente de uma URL\n",
    "abalone = pd.read_csv(url, header=None)\n",
    "\n",
    "# mostrar os 5 primeiros registros\n",
    "abalone.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os nomes das colunas estão no arquivo abalone.names, disponível no [repositório](https://archive.ics.uci.edu/ml/datasets/abalone). Podemos adicioná-los no dataframe da seguinte maneira:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "abalone.columns = [\n",
    "    \"Sex\",\n",
    "    \"Length\",\n",
    "    \"Diameter\",\n",
    "    \"Height\",\n",
    "    \"Whole weight\",\n",
    "    \"Shucked weight\",\n",
    "    \"Viscera weight\",\n",
    "    \"Shell weight\",\n",
    "    \"Rings\",\n",
    "]\n",
    "abalone.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como o objetivo é estimar a idade baseado apenas em dados físicos, podemos eliminar a coluna sexo usando o comando `.drop`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "abalone = abalone.drop(\"Sex\", axis=1)\n",
    "abalone.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estatísticas descritivas do dataset Abalone\n",
    "\n",
    "Uma abordagem interessante quando trabalhamos com machine learning e dados no geral é analisar os dados para ter uma idea de com o que estamos trabalhando. \n",
    "\n",
    "Como nosso `target` ou rótulo nesse caso são os aneis (coluna _ring_), podemos começar analisando isso. Um histograma nos dá uma visão geral do intervalo de idades que iremos encontrar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "abalone[\"Rings\"].hist(bins=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O histograma mostra que a grande maioria das conchas tem entre 5 e 15 anéis, mas também existem algumas com 25 ou mais. Os abalones mais velhos tem poucas amostras nesse dataset, o que parece intuitivo, uma vez que a distribuição das idades é geralmente enviesada nesse sentido em processos naturais.\n",
    "\n",
    "Uma segunda exploração que pode ser relevante seria encontrar quais variáveis podem ter uma correlação forte com a idade. Uma correlação forte entre uma variável independente e o nosso rótulo pode ser um bom dinal, e confirmaria que medidas físicas e idade estão relacionadas.\n",
    "\n",
    "Podemos observar a matrix de correlação completa. As correlações mais importantes são aquelas com a variável `Rings`, que será o nosso rótulo. Podemos obter essas correlações da seguinte maneira:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix = abalone.corr()\n",
    "correlation_matrix[\"Rings\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Olhando os coeficientes de correlação com a variável `Rings`, podemos concluir que existe ao menos alguma corrlação entre as medidas e a idade, ainda que não sejam muito altas (correlações mais altas indicam classificações mais simples).\n",
    "\n",
    "Pandas oferece muitas outras possibilidades de exploração de dados. Esse [link](https://realpython.com/pandas-python-explore-dataset/) contém varios exemplos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construindo um classificador $K$-NN passo-a-passo\n",
    "\n",
    "Uma diferença do $K$-NN em relação à maioria dos algoritmos de machine learning é que os calculos para ajustar o modelo são executados no momento da predição em vez da etapa de treinamento. Quando novos pontos são apresentados, como o nome indica, ele encontra os vizinhos mais próximos e atribui a ele o rótulo da maioria.\n",
    "\n",
    "A ideia geralmente faz sentido, quando pensamos que provavelmente nossos vizinhos mais próximos podem ter uma situação socio-econômica similar, pode levar os filhos na mesma escola, etc. Em outras nem tanto, como predizer sua cor favorita baseado na cor favorita de seus vizinhos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definindo \"Mais próximo\" matematicamente\n",
    "\n",
    "Para encontrar o vizinho mais próximo, a formulação matemática mais simples provavelmente é usando a distância Euclidiana. Considerando que estamos lidando com distância entre vetores, podemos representar da seguinte maneira:\n",
    "\n",
    "<img src=\"assets/distancia.png\" width=\"500\"/>\n",
    "\n",
    "Na imagem temos o ponto azul em (2,2) e verde em (4,4). Para computar a distância entre eles, podemos começar computando a diferença entre os vetores em cada eixo, o que forma um angulo de 90̣° (um triângulo). A linha conhectando os vetores azul e verde é a hipotenusa desse triângulo, que pode ser computada usando o teorema de pitágoras para nosso caso 2-D.\n",
    "\n",
    "Generalizando para casos de maior dimensionalidade, o comprimento desse vetor (no caso, a hipotenusa) é chamado de norma. A norma é um valor positivo que indica a magnitude de um vetor, e pode ser computado da seguinte maneira:\n",
    "\n",
    "\\begin{equation}\n",
    "d(a,b) = \\sqrt{(a_1-b_1)^2 + (a_2-b_2)^2 + \\dots + (a_n-b_n)^2}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A formula computa a distância considerando a diferença ao quadrado em cada uma das dimensões e depois tirando a raiz quadrada da soma de todos.\n",
    "\n",
    "Em Python, podemos fazer isso utilizando a função `linalg.norm()` do numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a = np.array([2, 2])\n",
    "b = np.array([4, 4])\n",
    "np.linalg.norm(a - b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encontrando o vizinho mais próximo\n",
    "\n",
    "Agora que sabemos computar a distância entre qualquer par de pontos, podemos usar isso para encontrar os vizinhos mais próximos de uma amostra de teste.\n",
    "\n",
    "Voltando ao problema do dataset Abalone, vamos converter nosso DataFrame Pandas para gerar um conjunto de dados e computar o $K$-NN. Podemos converter um DataFrame Pandas para numpy usando o atributo .values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = abalone.drop(\"Rings\", axis=1)\n",
    "X = X.values\n",
    "y = abalone[\"Rings\"]\n",
    "y = y.values\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em vez de separar em treinamento e teste, vamos simplesmente criar uma amostra para teste \"na mão\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data_point = np.array([\n",
    "    0.569552,\n",
    "    0.446407,\n",
    "    0.154437,\n",
    "    1.016849,\n",
    "    0.439051,\n",
    "    0.222526,\n",
    "    0.291208,\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em seguida computamos a distância entre esse ponto e cada ponto no dataset Abalone:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = np.linalg.norm(X - new_data_point, axis=1)\n",
    "\n",
    "# Uma distância para cada amostra\n",
    "distances.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Temos um vetor de distâncias, e precisamos encontrar os $k=3$ vizinhos mais próximos. Para isso, precisamos encontrar os IDs das menores distâncias. Podemos usar o metodo `.argsort()` para ordenar o array do menor para o maior, e pegar os índices dos $k$ menores elementos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 3\n",
    "nearest_neighbor_ids = distances.argsort()[:k]\n",
    "nearest_neighbor_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esses IDs nos mostram os 3 vizinhos mais próximos do nosso ponto `new_data_point`. Na sequência, vamos ver como estimar o seu valor baseado na votação entre esses vizinhos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Votando ou tirando a média de múltiplos vizinhos\n",
    "\n",
    "Após identificar os indices dos vizinhos mais próximos da nossa lesma do mar de idade desconhecida, combinamos o `target` dos vizinhos para predizer o do nosso ponto.\n",
    "\n",
    "Primeiramente, vamos analizar qual é o rótulo de cada vizinho:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "nearest_neighbor_rings = y[nearest_neighbor_ids]\n",
    "nearest_neighbor_rings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora que sabemos os valores das idades dos 3 vizinhos, podemos combiná-los numa predição para nosso ponto. Essa combinação é diferente para tarefas de regressão e classificação."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regressão - computando a média\n",
    "\n",
    "Problemas de regressão tem como saída um valor numérico. Nesse sentido, a predição é dada pela média dos valores dos rotulos dos vizinhos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = nearest_neighbor_rings.mean()\n",
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moda para classificação\n",
    "\n",
    "Problemas de classificação esperam como saída predições categóricas. Nesse caso, em vez de calcular a média, podemos computar a moda, i.e., o valor que mais ocorre entre todos os $k$ vizinhos mais próximos.\n",
    "\n",
    "Se existem multiplas modas, existem multiplas soluções possíveis, e teremos que escolher a solução vencedora de forma aleatória. Poderiamos também considerar a distância entre os vizinhos, em que a moda dos vizinhos mais próximos prevalecem.\n",
    "\n",
    "Podemos computar a moda usando a função `mode()` do SciPy. Como o caso do Abalone não é um bom exemplo de classificação, podemos verificar o exemplo da função mode da seguinte forma:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "class_neighbors = np.array([\"A\", \"B\", \"B\", \"C\"])\n",
    "scipy.stats.mode(class_neighbors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usando $K-$-NN no scikit-learn\n",
    "\n",
    "Como de costume, primeiramente dividimos os dados em treinamento e teste:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=12345\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criando e ajustando o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "knn_model = KNeighborsRegressor(n_neighbors=3)\n",
    "\n",
    "knn_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecionando o modelo\n",
    "\n",
    "Uma boa métrica para avaliar o resultado de problemas é regressão é o [root-mean-square error (RMSE)](https://en.wikipedia.org/wiki/Root-mean-square_deviation), que é computado da seguinte forma:\n",
    "\n",
    "1. computa a diferença entre o valor real e o valor estimado de cada ponto.\n",
    "2. computa o quadrado de cada diferença.\n",
    "3. soma todas as diferenças ao quadrado.\n",
    "4. calcula a raiz quadrada dos valores somados.\n",
    "\n",
    "Para começar, podemos computar o RMSE sobre o conjunto de treinamento, para o qual geralmente é esperado um RMSE relativamente bom:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "train_preds = knn_model.predict(X_train)\n",
    "mse = mean_squared_error(y_train, train_preds)\n",
    "rmse = sqrt(mse)\n",
    "rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, para uma resultado mais realista, vamos computar o RMSE sobre o conjunto de teste:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = knn_model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, test_preds)\n",
    "rmse = sqrt(mse)\n",
    "rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotando os resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "cmap = sns.cubehelix_palette(as_cmap=True)\n",
    "f, ax = plt.subplots()\n",
    "points = ax.scatter(\n",
    "    X_test[:, 0], X_test[:, 1], c=test_preds, s=50, cmap=cmap\n",
    ")\n",
    "f.colorbar(points)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nesse caso, estamos plotando as 2 primeiras colunas do dataset Abalone, i.e., comprimento e diâmetro. Podemos ver que essas 2 características estão fortemente correlacionadas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos confirmar essa tendência olhando os `targets` reais do dataset Abalone, simplesmente trocando `test_preds` por `y_test` no parâmetro c:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = sns.cubehelix_palette(as_cmap=True)\n",
    "f, ax = plt.subplots()\n",
    "points = ax.scatter(\n",
    "    X_test[:, 0], X_test[:, 1], c=y_test, s=50, cmap=cmap\n",
    ")\n",
    "f.colorbar(points)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lembre que temos 7 dimensões nesse dataset. Fique a vontade para testar outras combinações e verificar se as demais características também estão correlacionadas entre si e entre a idade da lesma do mar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercícios:\n",
    "\n",
    "1. Tente melhorar a performance do modelo otimizando o hiperparâmetro $k$ utilizando um grid search. Utilize valores de $k$ entre 1 e 50.\n",
    "\n",
    "2. Otimize o modelo através de um grid search considerando também a média ponderada baseada na distância entre as amostras. Considere também valores de $k$ entre 1 e 50."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit ('torchEnv': conda)",
   "language": "python",
   "name": "python37764bittorchenvconda44a76b6c90fa48eea1939354f840259c"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
