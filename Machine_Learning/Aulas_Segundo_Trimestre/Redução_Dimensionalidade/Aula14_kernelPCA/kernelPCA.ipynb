{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7689bdea-22ce-4d93-94d4-7cc0b61a6ef8",
   "metadata": {},
   "source": [
    "# Kernel Principal Component Analysis\n",
    "\n",
    "Muitos algoritmos de aprendizagem de maquinas assumem que os dados possam ser separados com uma funcao linear. O perceptron mesmo requer que os dados sejam perfeitamente separaveis para que o treinamento convirja. Outros algoritmos, como a regressao logistica, consideram a que a falta de separabilidade linear se deva a ruidos e anomalias. No entanto, quando lidamos com problemas nao lineares, os quais sao encontrados com bastante frequencia em aplicacoes reais, reducao de dados utilizando tecnicas lineares, como PCA e LDA, podem nao funcionar.\n",
    "\n",
    "Nessa aula, veremos como a versao _kernelizada_ do PCA, ou KPCA, transforma dados nao linearmente separaveis em um sub-espaco de menor dimensao onde os dados podem ser linearmente separaveis. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe7ba88-9aa6-4f16-9459-1f98ab05f69a",
   "metadata": {},
   "source": [
    "## Implementando o KPCA\n",
    "\n",
    "No exemplo a seguir, utilizaremos os pacotes SciPy e NumPy para implementar nossa versao do KPCA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "912e267c-76f7-4403-b3a1-6fa82d97cc38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy import exp\n",
    "from scipy.linalg import eigh\n",
    "import numpy as np \n",
    "def rbf_kernel_pca(X, gamma, n_components):\n",
    "    \"\"\"\n",
    "    Implementacao do PCA com kernel RBF (radial basis function).    \n",
    "    Parameters\n",
    "    ------------\n",
    "    X: {NumPy ndarray}, shape = [n_examples, n_features]  \n",
    "    gamma: float\n",
    "        Parametro de ajuste do kernel RBF\n",
    "    n_components: int\n",
    "        Numero de componentes (features) a ser retornado    \n",
    "    Retorna\n",
    "    ------------\n",
    "    X_pc: {NumPy ndarray}, shape = [n_examples, k_features]\n",
    "        Dataset projetado no novo espaco  \n",
    "    \"\"\"\n",
    "    # Computa a distancia euclidiana entre todos os pares de amostras\n",
    "    # no espaco original.\n",
    "    sq_dists = pdist(X, 'sqeuclidean')    \n",
    "    # Converte as distancias entre os pares em uma matriz quadrada.\n",
    "    mat_sq_dists = squareform(sq_dists)    \n",
    "    # Computa a matriz de kernels simetricos.\n",
    "    K = exp(-gamma * mat_sq_dists)    \n",
    "    # centraliza a matriz de kernels.\n",
    "    N = K.shape[0]\n",
    "    one_n = np.ones((N,N)) / N\n",
    "    K = K - one_n.dot(K) - K.dot(one_n) + one_n.dot(K).dot(one_n)    \n",
    "    # Obtem os 'autopares' da matriz de kernels centralizada\n",
    "    # a funcao scipy.linalg.eigh retorna esses valores em ordem ascendenter\n",
    "    eigvals, eigvecs = eigh(K)\n",
    "    eigvals, eigvecs = eigvals[::-1], eigvecs[:, ::-1]    \n",
    "    # Pega os k primeiros autovetores na nova projecao\n",
    "    X_pc = np.column_stack([eigvecs[:, i]\n",
    "                           for i in range(n_components)])    \n",
    "    return X_pc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4504359e-c88e-4b9b-bb21-80fb730b62bb",
   "metadata": {},
   "source": [
    "Uma desvantagem de usar o PCA com kernel RBF para reducao de dimensionalidade e que precisamos especificar o parametro gamma a priori. Encontrar o valor apropriado requer experimentar diversos valores, e talvez usar alguma tecnica de otimizacao de parametros, como por exemplo grid search."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee23f5e-9605-4f3d-a051-7a6dce7d8bb8",
   "metadata": {},
   "source": [
    "## Examplo â€” separando dados em formato de meia luas\n",
    "\n",
    "Vamos aplicar nosso PCA com kernel RBF em um dataset nao linear. Comecamos criando um dataset bi-dimensional com 100 amostras representando duas formas de meia lua:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a7d66732-dc41-4858-9782-961758abf409",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "import matplotlib.pyplot as plt\n",
    "X, y = make_moons(n_samples=100, random_state=123)\n",
    "plt.scatter(X[y==0, 0], X[y==0, 1],\n",
    "    color='red', marker='^', alpha=0.5)\n",
    "plt.scatter(X[y==1, 0], X[y==1, 1],\n",
    "    color='blue', marker='o', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57fef2a-a8dd-46d8-aca9-3458cf11fe51",
   "metadata": {},
   "source": [
    "Para uma melhor visualizacao, a meia lua de triangulos representa uma classe enquanto os circulos representam a outra classe.\n",
    "\n",
    "Podemos ver claramente que as duas meia luas nao sao linearmente separaveis, e nosso objetivo e desdobrar as as meia luas via KPCA para que o dataset possa servir como uma entrada boa o suficiente para um classificador linear. Mas primeiramente, vamos ver como fica o dataset se fizermos a projecao utilizando o PCA tradicional:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83eade9a-681e-473a-aea9-ef9311fde4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "scikit_pca = PCA(n_components=2)\n",
    "X_spca = scikit_pca.fit_transform(X)\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(7,3))\n",
    "ax[0].scatter(X_spca[y==0, 0], X_spca[y==0, 1],\n",
    "              color='red', marker='^', alpha=0.5)\n",
    "ax[0].scatter(X_spca[y==1, 0], X_spca[y==1, 1],\n",
    "              color='blue', marker='o', alpha=0.5)\n",
    "ax[1].scatter(X_spca[y==0, 0], np.zeros((50,1))+0.02,\n",
    "              color='red', marker='^', alpha=0.5)\n",
    "ax[1].scatter(X_spca[y==1, 0], np.zeros((50,1))-0.02,\n",
    "              color='blue', marker='o', alpha=0.5)\n",
    "ax[0].set_xlabel('PC1')\n",
    "ax[0].set_ylabel('PC2')\n",
    "ax[1].set_ylim([-1, 1])\n",
    "ax[1].set_yticks([])\n",
    "ax[1].set_xlabel('PC1')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa6059b-58b2-48f9-a644-5b77cf6bed4e",
   "metadata": {},
   "source": [
    "A projecao acima deixa bem claro que um classificador linear nao seria capaz de executar uma predicao adequada dos dados transformados utilizando o PCA tradicional.\n",
    "\n",
    "Notem que ao plotarmos apenas os componentes principais (a direita), nos deslocamos os triangulos um pouquinho pra cima e os circulos um pouquinho pra baixo para conseguir visualizar melhor a sobreposicao das classes. Essa transformacao nao ajudaria um classificador linear a discriminar entre os circulos e triangulos. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b287926-b8c1-4b1b-ae76-f5c8bff838bc",
   "metadata": {},
   "source": [
    "Agora vamos tentar com nosso PCA com kernel RBF, implementado acima:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "30748899-89f5-4f9c-ad53-cad88ce1e44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_kpca = rbf_kernel_pca(X, gamma=15, n_components=2)\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(7, 3))\n",
    "ax[0].scatter(X_kpca[y==0, 0], X_kpca[y==0, 1],\n",
    "              color='red', marker='^', alpha=0.5)\n",
    "ax[0].scatter(X_kpca[y==1, 0], X_kpca[y==1, 1],\n",
    "              color='blue', marker='o', alpha=0.5)\n",
    "ax[1].scatter(X_kpca[y==0, 0], np.zeros((50,1))+0.02,\n",
    "              color='red', marker='^', alpha=0.5)\n",
    "ax[1].scatter(X_kpca[y==1, 0], np.zeros((50,1))-0.02,\n",
    "              color='blue', marker='o', alpha=0.5)\n",
    "ax[0].set_xlabel('PC1')\n",
    "ax[0].set_ylabel('PC2')\n",
    "ax[1].set_ylim([-1, 1])\n",
    "ax[1].set_yticks([])\n",
    "ax[1].set_xlabel('PC1')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00cb730-906e-407e-8343-d405c2d4a57e",
   "metadata": {},
   "source": [
    "Podemos observar que as duas classes (circulos e triangulos) sao linearmente separaveis, de modo que temos um dataset adequado para treinar um classificador linear."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277749fc-f385-4db7-b6d2-8e6aec81dfda",
   "metadata": {},
   "source": [
    "Infelizmente, nao existe um valor universal para o parametro `gamma` que funcione bem em qualquer dataset. Encontrar esse valor requer experimentar diferentes valores. Nesse caso, um valor de `gamma=15` foi capaz de proporcionar um bom resultado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04605d27-bb67-4031-b348-5b05a8e92eab",
   "metadata": {},
   "source": [
    "## Conclusoes:\n",
    "\n",
    "Nessa aula nos implementamos o PCA com Kernel RBF em Python. Usando o `kernel trick` e uma projecao temporaria em um espaco de maior dimensao, o algoritmo e capaz de comprimir datasets compostos de features nao lineares em um sub-espaco de menor dimensionalidade onde as amostras de classes diferentes sao linearmente separaveis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c67fdd-0696-4b18-86eb-bb340f53d465",
   "metadata": {},
   "source": [
    "# Exercicios\n",
    "\n",
    "Considerando o seguinte dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3382b1b-e5ec-411c-96aa-7c32f226efa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_circles\n",
    "X, y = make_circles(100, factor=.1, noise=.1)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1638c3-d879-4e36-b9ad-9c6a966b30cc",
   "metadata": {},
   "source": [
    "1. Transforme o dataset a seguir usando KPCA. Encontre o valor de `gamma` que maximiza a separabilidade. \n",
    "2. Transforme o mesmo dataset para uma menor dimensionalidade utilizando PCA e LDA. Utilize as transformacoes para alimentar um algoritmo de classificacao linear (Perceptron, regressao logistica, etc). Compare os resultaddos com o KPCA otimizado."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
