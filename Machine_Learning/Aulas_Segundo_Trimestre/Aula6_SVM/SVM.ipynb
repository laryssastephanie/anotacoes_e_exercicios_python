{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM\n",
    "\n",
    "Máquinas de Vetores de Suporte (_Support vector machines (SVMs)_) representam uma classe de algoritmos de aprendizagem supervisionada particularmente poderosa e flexivel, que pode ser usada tanto para classificação quanto para regressão. Nesta aula, vamos abordar os conceitos por trás do SVM e sua aplicação para problemas de classificação.\n",
    "\n",
    "Começando com os imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "# usando as configurações padrões do seaborn para plotagem\n",
    "import seaborn as sns; sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivação do SVM\n",
    "\n",
    "O SVM é um modelo discriminativo, i.e., tenta encontrar uma linha ou curva (para casos 2D) ou hyperplano ou _manifold_ (para casos com mais dimensões) que melhor separe as classes de um problema. \n",
    "\n",
    "Vamos ilustrar com um exemplo contendo duas classes de pontos bem separados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "X, y = make_blobs(n_samples=50, centers=2,\n",
    "                  random_state=0, cluster_std=0.60)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um classificador linear poderia tentar tração uma linha reta separando os 2 conjuntos, e consequentemente criando um modelo de classificação. Nesse caso simples em 2D, é possível traçar essa reta à mão, mas logo surge uma dúvida: existe mais de uma possivel linha separando perfeitamente esses dados:\n",
    "\n",
    "Podemos desenhar algumas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "xfit = np.linspace(-1, 3.5)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n",
    "plt.plot([0.6], [2.1], 'x', color='red', markeredgewidth=2, markersize=10)\n",
    "\n",
    "for m, b in [(1, 0.65), (0.5, 1.6), (-0.2, 2.9)]:\n",
    "    plt.plot(xfit, m * xfit + b, '-k')\n",
    "\n",
    "plt.xlim(-1, 3.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nesse caso, temos três retas bem diferentes, que no entando discriminam perfeitamente essas amostras. Dependendo de qual linha você escolhe, uma nova amostra a ser classificada (ex: o \"X\" no plot) pode receber diferentes rótulos! Fica evidente que simplesmente \"desenhar uma linha entre as classes\" não é suficiente e precisamos de uma abordagem melhor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximizando as margens\n",
    "\n",
    "SVMs oferecem uma alternativa para melhorar esse cenário. A idéia é a seguinte: em vez de simplesmente traçar uma reta com largura zero entre as 2 classes, podemos traçar também uma margem com uma certa dimensão, que vai até o ponto mais próximo. Veja um exemplo de como essa margem poderia ser:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "xfit = np.linspace(-1, 3.5)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n",
    "\n",
    "for m, b, d in [(1, 0.65, 0.33), (0.5, 1.6, 0.55), (-0.2, 2.9, 0.2)]:\n",
    "    yfit = m * xfit + b\n",
    "    plt.plot(xfit, yfit, '-k')\n",
    "    plt.fill_between(xfit, yfit - d, yfit + d, edgecolor='none',\n",
    "                     color='#AAAAAA', alpha=0.4)\n",
    "\n",
    "plt.xlim(-1, 3.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A reta que maximiza essa margem será a escolhida como modelo ótimo. Sendo assim, SVM pode ser visto como um exemplo de estimador que tenta maximizar o tamanho das margens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinando o SVM\n",
    "\n",
    "Veremos o resultado após ajustar a reta a esses dados: para isso, usaremos o support vector classifier do já conhecido Scikit-Learn para treinar um modelo SVM. Utilizaremos um kernel linear e setaremos o parâmetro C para um número bem grande (esses significado desses parâmetros serão discutidos logo em seguida)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC # \"Support vector classifier\"\n",
    "model = SVC(kernel='linear', C=1E10)\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para visualizar melhor o que está ocorrendo aqui, criaremos uma função que plota o limite de decisão:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_svc_decision_function(model, ax=None, plot_support=True):\n",
    "    \"\"\"Plota a função de decisão para um classificador SVM 2D\"\"\"\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    xlim = ax.get_xlim()\n",
    "    ylim = ax.get_ylim()\n",
    "    \n",
    "    # Cria um grid para avaliar o modelo\n",
    "    x = np.linspace(xlim[0], xlim[1], 30)\n",
    "    y = np.linspace(ylim[0], ylim[1], 30)\n",
    "    Y, X = np.meshgrid(y, x)\n",
    "    xy = np.vstack([X.ravel(), Y.ravel()]).T\n",
    "    P = model.decision_function(xy).reshape(X.shape)\n",
    "    \n",
    "    # plota a reta de decisão e as margens\n",
    "    ax.contour(X, Y, P, colors='k',\n",
    "               levels=[-1, 0, 1], alpha=0.5,\n",
    "               linestyles=['--', '-', '--'])\n",
    "    \n",
    "    # destaca os vetores de suporte\n",
    "    if plot_support:\n",
    "        ax.scatter(model.support_vectors_[:, 0],\n",
    "                   model.support_vectors_[:, 1],\n",
    "                   s=300, linewidth=2, facecolors='none',edgecolors='k');\n",
    "    ax.set_xlim(xlim)\n",
    "    ax.set_ylim(ylim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n",
    "plot_svc_decision_function(model);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essa é a reta que maximiza as margens entre os 2 conjuntos de pontos. Note que poucas amostras de treinamento tocam nessa margem, e estão marcados com um circulo em volta. Esses pontos são os elementos pivot nesse ajuste, e são chamados de vetores de suporte, os quais dão o nome ao algoritmo. No Sckit-learn, a localização desses pontos são armazenadas no atributo support_vectors_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.support_vectors_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um ponto chave no sucesso do SVM é que para classificação, apenas a posição dos vetores de suporte importam; quaisquer pontos além dessa margem que estejam do lado correto não modificam o ajuste. Tecnicamente, é por que esses demais pontos não contribuem para a função de _loss_ no ajuste do modelo, então suas posições não importam contanto que eles não cruzem as margens.\n",
    "\n",
    "Podemos observar isso, por exemplo, se plotarmos o modelo aprendido a partir dos primeiros 60 pontos e a partir dos primeiros 120 pontos do dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_svm(N=10, ax=None):\n",
    "    X, y = make_blobs(n_samples=200, centers=2,\n",
    "                      random_state=0, cluster_std=0.60)\n",
    "    X = X[:N]\n",
    "    y = y[:N]\n",
    "    model = SVC(kernel='linear', C=1E10)\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    ax = ax or plt.gca()\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n",
    "    ax.set_xlim(-1, 4)\n",
    "    ax.set_ylim(-1, 6)\n",
    "    plot_svc_decision_function(model, ax)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(16, 6))\n",
    "fig.subplots_adjust(left=0.0625, right=0.95, wspace=0.1)\n",
    "for axi, N in zip(ax, [60, 120]):\n",
    "    plot_svm(N, axi)\n",
    "    axi.set_title('N = {0}'.format(N))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No plot da esquerda, vemos o painel e os vetores de suporte para as 60 amostras de treinamento. No painel a plot da direita, dobramos o número de amostras de treinamento, mas o modelo não mudou: os três vetores de suporte do plot da esquerda são os mesmos 3 vetores de suporte da direita. Essa insensividade em relação aos pontos mais distantes é um dos pontos fortes do SVM.\n",
    "\n",
    "\n",
    "\n",
    "In the left panel, we see the model and the support vectors for 60 training points. In the right panel, we have doubled the number of training points, but the model has not changed: the three support vectors from the left panel are still the support vectors from the right panel. This insensitivity to the exact behavior of distant points is one of the strengths of the SVM model.\n",
    "\n",
    "Podemos ver esse resultado de modo interativo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact, fixed\n",
    "interact(plot_svm, N=[10, 50, 100, 200], ax=fixed(None));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Além das barreiras lineares: SVM com Kernel\n",
    "\n",
    "SVM se torna extremamente poderoso quando combinado com kernels. A utilização de kernels permite projetar nossos dados em dimensões mais altas, definidas por polinômios e funções de base Gaussianas, e assim capazes de ajustar relacionamentos não lineares usando classificação linear.\n",
    "\n",
    "Para termos uma ideia de como funciona, vamos olhar para um exemplo não linearmente separável."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets.samples_generator import make_circles\n",
    "X, y = make_circles(100, factor=.1, noise=.1)\n",
    "\n",
    "clf = SVC(kernel='linear').fit(X, y)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n",
    "plot_svc_decision_function(clf, plot_support=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esse exemplo deixa claro que nenhum classificador linear seja capaz de separar esses dados. No entanto, podemos utilizar mecanismos para projetar esses dados em um espaço de maior dimensão, de modo que um separador linear seja eficiente. Nesse exemplo, uma projeção simples seria computar a função de base radial centralizada no meio dos dados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = np.exp(-(X ** 2).sum(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos visualizar essa dimensão extra usando um plot 3D:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits import mplot3d\n",
    "\n",
    "def plot_3D(elev=30, azim=30, X=X, y=y):\n",
    "    ax = plt.subplot(projection='3d')\n",
    "    ax.scatter3D(X[:, 0], X[:, 1], r, c=y, s=50, cmap='autumn')\n",
    "    ax.view_init(elev=elev, azim=azim)\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "    ax.set_zlabel('r')\n",
    "\n",
    "interact(plot_3D, elev=[-90,-45,-30,-0,30,45,90,135, 180], azip=(-180, 180),\n",
    "         X=fixed(X), y=fixed(y));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que nessa dimensão adicional, os dados se tornam linearmente separaveis com uma reta, digamos, com r=0.7.\n",
    "\n",
    "Aqui nós precisamos escolher e ajustar cuidadosamente a nossa projeção: se não houvessemos centralizado a função de base radial na localização correta, não teriamos obtido um resultado os dados pudessem ser separados de forma tão clara. Em geral, escolher essa posição se torna um problema: temos que encontrar, de forma automática, a melhor função de base a ser usada.\n",
    "\n",
    "Uma estratégia seria computar a função de base centralizada em cada amostra do dataset, e deixar o SVM peneirar o melhor resultado. Esse tipo de função de base é chamado de kernel, por ser baseada em relacionamentos de similaridade entre pares de amostras.\n",
    "\n",
    "Um problema em potencial com essa estratégia - projetar N pontos em N dimensões - é que pode ser muito custoso, em termos computacionais, quando N cresce para valores muito grandes. No entanto, um truque conhecido como _kernel trick_, que ajusta a transformação dos dados the forma implicita, i.e., sem precisar construir todas as possíveis representações N-dimensionais da projeção do kernel! Esse truque é implementado dentro do SVM, e essa é uma das razẽos de o SVM ser tão poderoso.\n",
    "\n",
    "No Scikit-Learn, podemos aplicar o SVM com kernel simplesmente mudando nosso kernel linear para kernel com função de base radial (_radial basis function (RBF)_), usando o parâmetro:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC(kernel='rbf', C=1E6)\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n",
    "plot_svc_decision_function(clf)\n",
    "plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1],\n",
    "            s=300, lw=1, facecolors='none');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Melhorando o SVM: Margens suaves\n",
    "\n",
    "Nossa discussão até agora considerou dados muito limpos, em que é possível definir uma linha de decisão muito clara. Mas e se nossos dados contêm sobreposição de classes? Podem ser, por exemplo, assim:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_blobs(n_samples=100, centers=2,\n",
    "                  random_state=0, cluster_std=1.2)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para lidar com casos assim, a implementação do SVM contém um 'fator de incertezas' que ``suaviza`` as margens, i.e., permite que alguns pontos 'rastegem' dentro da margem, se isso permitir um ajuste melhor. A rigidez da margem é controlada pelo hyperparâmetro $C$. Valores grandes de $C$ equivale a margens mais sólidas, em que as amostras não devem estar. Valores pequenos de $C$ correspondem a margens mais suaves, que podem abrigar alguns pontos.\n",
    "\n",
    "O plot abaixo permite uma interpretação visual de como mudanças em $C$ afetam o ajuste final através da suavização da margem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_blobs(n_samples=100, centers=2,\n",
    "                  random_state=0, cluster_std=0.8)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(16, 6))\n",
    "fig.subplots_adjust(left=0.0625, right=0.95, wspace=0.1)\n",
    "\n",
    "for axi, C in zip(ax, [10.0, 0.1]):\n",
    "    model = SVC(kernel='linear', C=C).fit(X, y)\n",
    "    axi.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n",
    "    plot_svc_decision_function(model, axi)\n",
    "    axi.scatter(model.support_vectors_[:, 0],\n",
    "                model.support_vectors_[:, 1],\n",
    "                s=300, lw=1, facecolors='none');\n",
    "    axi.set_title('C = {0:.1f}'.format(C), size=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O valor ótimo de $C$ vai depender do dataset, e deve ser escolhido por validação cruzada, ou outros métodos (veremos isso na próxima aula)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examplo: Reconhecimento Facial\n",
    "\n",
    "Como exemplo de SVM em ação, vamos dar uma olhada no problema de reconhecimento de faces. Nós usaremos o dataset rotulado _Faces in the Wild_, composto por milhares de fotos de várias figuras públicas. Podemos pegar esses dados usando Scikit-Learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_lfw_people\n",
    "faces = fetch_lfw_people(min_faces_per_person=60)\n",
    "print(faces.target_names)\n",
    "print(faces.images.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotando algumas amostras para ter ideia do contexto em que estamos trabalhando:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3, 5)\n",
    "for i, axi in enumerate(ax.flat):\n",
    "    axi.imshow(faces.images[i], cmap='bone')\n",
    "    axi.set(xticks=[], yticks=[],\n",
    "            xlabel=faces.target_names[faces.target[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada imagem contem 64x47 pixels (aproximadamente 3,000). Nós poderiamos simplesmente usar a intensidade de cada pixel como feature, mas geralmente, é mais eficiente usar alguns métodos de pré-processamente para extrair características mais significativas. Aqui, nós usaremos as técnicas _Principal Component Analysis (PCA)_ para extrair os 150 componentes mais relevantes e usá-los como features de cada amostra. Podemos fazer isso de modo simples juntando o pré-processamento e a classificação em um único _pipeline_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "pca = PCA(n_components=150, whiten=True, random_state=42)\n",
    "svc = SVC(kernel='rbf', class_weight='balanced')\n",
    "model = make_pipeline(pca, svc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para testar a saída de nosso classificador, vamos dividir os dados em treinamento e teste;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(faces.data, faces.target,\n",
    "                                                random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, podemos usar uma validação cruzada com grid search (busca exaustiva) para explorar a combinação dos parâmetros. Aqui, ajustaremos $C$ (que controla a suavidade das bordas) e gamma (que controla o tamanho do kernel da função de base radial) para encontrar o melhor modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {'svc__C': [1, 5, 10, 50],\n",
    "              'svc__gamma': [0.0001, 0.0005, 0.001, 0.005]}\n",
    "grid = GridSearchCV(model, param_grid)\n",
    "\n",
    "%time grid.fit(Xtrain, ytrain)\n",
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O valor ótimo fica entre os valores médios do nosso grid; se caisse nos extremos, seria interessante expandir esse grid para novos valores e nos certificarmos que escolhemos os valores ótimos.\n",
    "\n",
    "Agora que temos esse modelo validade, podemos estimar os rótulos do conjunto de teste, os quais ainda não foram vistos pelo modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = grid.best_estimator_\n",
    "yfit = model.predict(Xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos dar uma olhada a algumas das imagens com seus respectivos rótulos estimados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(4, 6)\n",
    "for i, axi in enumerate(ax.flat):\n",
    "    axi.imshow(Xtest[i].reshape(62, 47), cmap='bone')\n",
    "    axi.set(xticks=[], yticks=[])\n",
    "    axi.set_ylabel(faces.target_names[yfit[i]].split()[-1],\n",
    "                   color='black' if yfit[i] == ytest[i] else 'red')\n",
    "fig.suptitle('Rótulos (nomes) estimados; Rótulos errados em vermelho', size=14);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nessa pequena amostragem, o estimador ótimo classificou errado apenas uma face (Bush foi confundido com Blair). Podemos ter uma ideia melhor da performance do modelo usando o relatório de classificação, que lista as estatísticas para cada rótulo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(ytest, yfit,\n",
    "                            target_names=faces.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos mostrar também a matriz de confusão:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "mat = confusion_matrix(ytest, yfit)\n",
    "sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False,\n",
    "            xticklabels=faces.target_names,\n",
    "            yticklabels=faces.target_names,cmap=plt.cm.Blues)\n",
    "plt.xlabel('Nomes Reais')\n",
    "plt.ylabel('Nomes Preditos');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para casos reais de classificação de pessoas, onde o rosto já não esteja selecionado, precisariamos usar métodos mais eficientes de extração de características, ou então métodos mais poderosos, como _Deep Learning_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sumarizando\n",
    "\n",
    "SVM é uma técnica muito poderosa e foi uma das principais abordagens de aprendizado de máquinas por muito tempo, devido a diversos motivos:\n",
    "\n",
    "- depende de poucos vetores de suporte para classificação, o que resulta em modelos compactos com baixo consumo de memória.\n",
    "- uma vez treinado, a inferência é bem rápida.\n",
    "- por ser afetado apenas por amostras próximas às margens, são eficientes para dados com alta dimensionalidade, inclusive para datasets com mais dimensões do que dados.\n",
    "- a integração com os kernels faz o SVM muito versátil, capaz de se adaptar a muitos tipos de dados.\n",
    "\n",
    "No entanto, SVM também tem varias desvantagens:\n",
    "\n",
    "- a computação de um número muito grande de amostras pode ser proibitivo.\n",
    "- os resultados são muito dependentes de uma escolha apropriada do parâmetro de suavização $C$. \n",
    "- os resultados não permitem uma interpretação probabilística. É possível computar o SVM probabilístico, porém, o processo é custoso.\n",
    "\n",
    "Considerando esses pontos, existem modelos mais rápidos para problemas mais simples. No entanto, SVM pode providenciar excelentes resultados para determinados problemas em tempo hábil."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercício:\n",
    "\n",
    "1. Baixar alguns datasets de classificação do repositório [Toy datasets](https://scikit-learn.org/stable/datasets/toy_dataset.html) do Scikit-Learn e classificá-los usando SVM. Verificar se funciona melhor com SVM linear ou com kernel RBF, e testar diferentes valores de $C$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit ('torchEnv': conda)",
   "language": "python",
   "name": "python37764bittorchenvconda44a76b6c90fa48eea1939354f840259c"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
