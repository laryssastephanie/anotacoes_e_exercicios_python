{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Redes Neurais Multilayer Perceptron (MLP) com PyTorch\n",
    "\n",
    "Redes Neurais podem apresentar um número massivo de parâmetros com dezenas de camadas a serem aprendidas (nesse caso, chamamos de ''deep learning''). Pora ajudar a construir essas redes, o PyTorch possui o módulo `nn`, que contêm diversas ferramentas para construir redes neurais de forma eficiente."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# Importando os pacotes necessários\n",
    "\n",
    "# comandos para plotar imagens mais bem definidas no notebook\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Algumas funções auxiliares\n",
    "import helper\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Como exemplo, vamos mais uma vez utilizar o já conhecido dataset MNIST, formado por imagens de digitos em tons de cinza de dimensão $28\\times28$, apresentados na imagem abaixo.\n",
    "\n",
    "\n",
    "\n",
    "<img src='assets/mnist.png'>\n",
    "\n",
    "Mais uma vez, vamos usar o pacote `torchvision` para carregar o dataset."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Define a transformação dos dados. \n",
    "# Note que nesse caso joga um valor médio de 0.5 e desvio de 0.5, normalizando os valores entre -1 e 1.\n",
    "# (https://discuss.pytorch.org/t/understanding-transform-normalize/21730)\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                              transforms.Normalize((0.5,), (0.5,)),\n",
    "                              ])\n",
    "\n",
    "# Download and load the training data\n",
    "trainset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Criamos o data loader de treinamento `trainloader` e criamos um _iterator_ `iter(trainloader)`, que será chamadado da seguinte forma:\n",
    "\n",
    "```python\n",
    "for image, label in trainloader:\n",
    "    ## do things with images and labels\n",
    "```\n",
    "\n",
    "Note que usamos batches de tamanho $64$. Aqui vamos consultar o primeiro batch para verificar os dados. Observe que `images` aqui é um vetor com tamanho `(64, 1, 28, 28)`, ou seja, 64 imagens por batch, $1$ cor por canal (se fossem imagens coloridas seriam 3 canais), e imagens de 28x28 pixels."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "print(type(images))\n",
    "print(images.shape)\n",
    "print(labels.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'torch.Tensor'>\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "plotando uma das imagens"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "plt.imshow(images[1].numpy().squeeze(), cmap='Greys_r');"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ],
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfcAAAHwCAYAAAC7cCafAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8rg+JYAAAACXBIWXMAABYlAAAWJQFJUiTwAAAc60lEQVR4nO3df7BudV0v8PcnKLiHFM9lSqa8DeIvmEy5HEqFEUEmLqYBJlwdp2IasR+XroF6yzHponnLaSwlUGxykgZHqEE9TkrqTX7/0OqQoZOKhAd0ghAQFA6Q6Pf+8axT5x73Ppz9PM/Za+/v83rNPLP2s9b6PN8Py+V57/Xs9aNaawEA+vF9YzcAAMyXcAeAzgh3AOiMcAeAzgh3AOiMcAeAzgh3AOiMcAeAzgh3AOiMcAeAzgh3AOiMcAeAzuw9dgN7QlV9Jcnjk2wduRUAmNZBSb7ZWnvySgu7DPdMgv0/Dy8AWCi9fi2/dewGAGAOtk5TNGq4V9WTqurPqupfquqRqtpaVe+sqo1j9gUA69loX8tX1VOSXJ/kh5N8JMkXk/xUkt9IckJVHdVau2es/gBgvRrzyP3dmQT7a1prJ7fW3tBae2GSdyR5RpL/M2JvALBuVWtt9QedHLXfksnfEp7SWvvuDssel+SOJJXkh1trD07x+VuSHD6fbgFgNDe21jattGisr+WPHaaf3DHYk6S19q2qui7J8Umem+RTy33IEOJLOWQuXQLAOjTW1/LPGKY3L7P8y8P06avQCwB0Zawj9/2H6f3LLN8+/wm7+pDlvqrwtTwAi6zX69wBYGGNFe7bj8z3X2b59vn37flWAKAvY4X7l4bpcn9Tf9owXe5v8gDAMsYK9yuG6fFV9f/1MFwKd1SSbUk+vdqNAcB6N0q4t9b+OcknM3nizRk7LX5zkv2SXDTNNe4AsOjGfCrc/8jk9rN/XFXHJflCkudkcg38zUl+e8TeAGDdGu1s+eHo/YgkF2YS6q9L8pQk5yZ5rvvKA8B0Rn2ee2vtq0l+acweAKA3rnMHgM4IdwDojHAHgM4IdwDojHAHgM4IdwDojHAHgM4IdwDojHAHgM4IdwDojHAHgM4IdwDojHAHgM4IdwDojHAHgM4IdwDojHAHgM4IdwDojHAHgM4IdwDojHAHgM4IdwDojHAHgM4IdwDojHAHgM4IdwDojHAHgM4IdwDojHAHgM4IdwDojHAHgM4IdwDojHAHgM4IdwDojHAHgM4IdwDojHAHgM4IdwDojHAHgM4IdwDojHAHgM4IdwDojHAHgM4IdwDojHAHgM7sPXYDwHgOO+ywqWv33nu2fz5++Zd/eab6Qw89dOrao446aqaxx3TnnXdOXfusZz1rprHvvvvumepZPY7cAaAzwh0AOiPcAaAzwh0AOiPcAaAzwh0AOiPcAaAzwh0AOiPcAaAzwh0AOiPcAaAzwh0AOiPcAaAzwh0AOiPcAaAznucOI9u4cePUtZs3b55p7COPPHLq2r322mumsWf1yCOPTF17/fXXz7GTlXnmM585U/2BBx44de0ZZ5wx09hvfvObZ6pn9Yx25F5VW6uqLfO6c6y+AGC9G/vI/f4k71xi/gOr3AcAdGPscL+vtXbOyD0AQFecUAcAnRn7yH2fqvr5JD+W5MEkNyW5urX2nXHbAoD1a+xwPzDJRTvN+0pV/VJr7arHKq6qLcssOmTmzgBgnRrza/n3JTkuk4DfL8lPJPmTJAcl+euqevZ4rQHA+jXakXtrbecLJj+f5Fer6oEkr0tyTpKXPsZnbFpq/nBEf/gc2gSAdWctnlD3nmF69KhdAMA6tRbD/evDdL9RuwCAdWothvtzh+mto3YBAOvUKOFeVYdW1fccmVfVQUnOH96+f1WbAoBOjHVC3cuTvK6qrk5yW5JvJXlKkhcn2TfJZUnePlJvALCujRXuVyR5RpL/muSoTP6+fl+SazO57v2i1lobqTcAWNdGCffhBjWPeZMaWC2zPHb1ta997Uxjz1I/6+/An/vc56auveGGG2Ya+x/+4R9mqv/whz88de0999wz09izuO6662aqf97znjenTujZWjyhDgCYgXAHgM4IdwDojHAHgM4IdwDojHAHgM4IdwDojHAHgM4IdwDojHAHgM4IdwDojHAHgM4IdwDojHAHgM4IdwDozCjPc4d5+73f+72Z6s8444w5dbJyp59++tS1F1988Rw7YXf9wA/8wNS1BxxwwBw7WZmvfvWro43N6nLkDgCdEe4A0BnhDgCdEe4A0BnhDgCdEe4A0BnhDgCdEe4A0BnhDgCdEe4A0BnhDgCdEe4A0BnhDgCdEe4A0JlqrY3dw9xV1ZYkh4/dB6vngQcemKn+2muvnbr21a9+9Uxjewzn+nPZZZdNXXvCCSfMNPYdd9wxde2P/uiPzjQ2o7ixtbZppUWO3AGgM8IdADoj3AGgM8IdADoj3AGgM8IdADoj3AGgM8IdADoj3AGgM8IdADoj3AGgM8IdADoj3AGgM8IdADoj3AGgM3uP3QDMww/+4A+O3QLryMaNG2eqP/bYY+fUycp99KMfHW1s1g9H7gDQGeEOAJ0R7gDQGeEOAJ0R7gDQGeEOAJ0R7gDQGeEOAJ0R7gDQGeEOAJ0R7gDQGeEOAJ0R7gDQGeEOAJ3xyFdgXZrlsa3XXHPNTGPvs88+U9e++93vnmnsN7zhDTPVsxgcuQNAZ+YS7lV1SlWdV1XXVNU3q6pV1fsfo+bIqrqsqu6tqoeq6qaqOrOq9ppHTwCwqOb1tfybkjw7yQNJvpbkkF2tXFUnJflgkoeT/EWSe5P8bJJ3JDkqyalz6gsAFs68vpY/K8nTkzw+ya/tasWqenySP03ynSTHtNZe1Vr7X0kOS3JDklOq6hVz6gsAFs5cwr21dkVr7cuttbYbq5+S5IeSXNJa+/sdPuPhTL4BSB7jFwQAYHljnFD3wmH68SWWXZ1kW5Ijq2r601EBYIGNcSncM4bpzTsvaK09WlVfSfLjSQ5O8oVdfVBVbVlm0S7/5g8APRvjyH3/YXr/Msu3z3/Cnm8FAPqzrm9i01rbtNT84Yj+8FVuBwDWhDGO3Lcfme+/zPLt8+/b860AQH/GCPcvDdOn77ygqvZO8uQkjya5dTWbAoBejBHulw/TE5ZYdnSSDUmub609snotAUA/xgj3S5PcneQVVXXE9plVtW+Stw5vLxihLwDowlxOqKuqk5OcPLw9cJg+r6ouHH6+u7X2+iRprX2zql6dSchfWVWXZHL72RMzuUzu0kxuSQsATGFeZ8sfluS0neYdPLyS5LYkr9++oLW2uapekOS3k7wsyb5Jbkny2iR/vJt3ugMAllA95qhL4WDt27Bhw0z1H/jAB6auPfHEE2ca+6GHHpq69uijj55p7C1blrt3F526cbnLvnfF89wBoDPCHQA6I9wBoDPCHQA6I9wBoDPCHQA6I9wBoDPCHQA6I9wBoDPCHQA6I9wBoDPCHQA6I9wBoDPCHQA6M6/nuQOsyLnnnjtT/UknnTR17YMPPjjT2C95yUumrvXIVlaDI3cA6IxwB4DOCHcA6IxwB4DOCHcA6IxwB4DOCHcA6IxwB4DOCHcA6IxwB4DOCHcA6IxwB4DOCHcA6IxwB4DOCHcA6IznuQNTeeUrXzlT/ctf/vKZ6h966KGpa08++eSZxr7yyitnqoc9zZE7AHRGuANAZ4Q7AHRGuANAZ4Q7AHRGuANAZ4Q7AHRGuANAZ4Q7AHRGuANAZ4Q7AHRGuANAZ4Q7AHRGuANAZzzyFRbYG9/4xqlr3/rWt8409sMPPzxT/W/+5m9OXfs3f/M3M40Na50jdwDojHAHgM4IdwDojHAHgM4IdwDojHAHgM4IdwDojHAHgM4IdwDojHAHgM4IdwDojHAHgM4IdwDojHAHgM4IdwDojOe5w8g2btw4de3mzZtnGvvII4+cunbbtm0zjf2Wt7xlpvrzzz9/pnromSN3AOjMXMK9qk6pqvOq6pqq+mZVtap6/zLrHjQsX+51yTx6AoBFNa+v5d+U5NlJHkjytSSH7EbNPybZvMT8z8+pJwBYSPMK97MyCfVbkrwgyRW7UfPZ1to5cxofABjMJdxba/8e5lU1j48EAKY05tnyP1JVv5LkgCT3JLmhtXbTSj6gqrYss2h3/iwAAF0aM9x/enj9u6q6MslprbXbR+kIADowRrhvS/K7mZxMd+sw71lJzklybJJPVdVhrbUHH+uDWmublpo/HNEfPo9mAWC9WfXr3Ftrd7XWfqe1dmNr7b7hdXWS45N8JslTk5y+2n0BQC/WzE1sWmuPJnnv8PboMXsBgPVszYT74OvDdL9RuwCAdWythftzh+mtu1wLAFjWqod7VR1eVd8zblUdl8nNcJJkyVvXAgCPbS5ny1fVyUlOHt4eOEyfV1UXDj/f3Vp7/fDzHyV5WlVdn8ld7ZLJ2fIvHH4+u7V2/Tz6AoBFNK9L4Q5LctpO8w4eXklyW5Lt4X5Rkpcm+ckkL0ry/Un+NclfJjm/tXbNnHoCgIVUrbWxe5g717mzmjZs2DBT/Qc+8IGpa0888cSZxv72t789de2ZZ54509gXXHDBTPWwIG5c7p4uu7LWTqgDAGYk3AGgM8IdADoj3AGgM8IdADoj3AGgM8IdADoj3AGgM8IdADoj3AGgM8IdADoj3AGgM8IdADoj3AGgM/N6njusa0ccccTUtX/+538+09iHHnro1LXbtm2baeyXvOQlU9deeeWVM43NYnnxi188U/2NN944de0dd9wx09jrkSN3AOiMcAeAzgh3AOiMcAeAzgh3AOiMcAeAzgh3AOiMcAeAzgh3AOiMcAeAzgh3AOiMcAeAzgh3AOiMcAeAzgh3AOiM57nThQ0bNsxU/6EPfWjq2ic96Ukzjf2Nb3xj6tpNmzbNNPbWrVtnqmd9Oe+882aqP+WUU6auveqqq2Ya+7rrrpupftE4cgeAzgh3AOiMcAeAzgh3AOiMcAeAzgh3AOiMcAeAzgh3AOiMcAeAzgh3AOiMcAeAzgh3AOiMcAeAzgh3AOiMR76yZmzcuHHq2s985jMzjT3LY1v/7u/+bqaxn/Oc58xUz8rNsq8lyWGHHTZ17aWXXjrT2LP0ftddd8009llnnTV17cUXXzzT2KyMI3cA6IxwB4DOCHcA6IxwB4DOCHcA6IxwB4DOCHcA6IxwB4DOCHcA6IxwB4DOCHcA6IxwB4DOCHcA6IxwB4DOCHcA6IznubNmnH322VPXPvWpT51p7E9/+tNT1x5//PEzjb2oXvnKV85Uf/DBB09de8YZZ8w09hOf+MSpa7dt2zbT2O9617umrn37298+09i33XbbTPWsnpmP3KvqgKo6vao+XFW3VNVDVXV/VV1bVa+qqiXHqKojq+qyqrp3qLmpqs6sqr1m7QkAFtk8jtxPTXJBkjuSXJHk9iRPTPJzSd6b5EVVdWprrW0vqKqTknwwycNJ/iLJvUl+Nsk7khw1fCYAMIV5hPvNSU5M8rHW2ne3z6yqNyb52yQvyyToPzjMf3ySP03ynSTHtNb+fph/dpLLk5xSVa9orV0yh94AYOHM/LV8a+3y1tpf7Rjsw/w7k7xneHvMDotOSfJDSS7ZHuzD+g8nedPw9tdm7QsAFtWePlv+28P00R3mvXCYfnyJ9a9Osi3JkVW1z55sDAB6tcfOlq+qvZP84vB2xyB/xjC9eeea1tqjVfWVJD+e5OAkX3iMMbYss+iQlXULAP3Yk0fub0vyzCSXtdY+scP8/Yfp/cvUbZ//hD3UFwB0bY8cuVfVa5K8LskXk/zCnhgjSVprm5YZf0uSw/fUuACwls39yL2qfj3JuUn+KcmxrbV7d1pl+5H5/lna9vn3zbs3AFgEcw33qjozyXlJPp9JsN+5xGpfGqZPX6J+7yRPzuQEvFvn2RsALIq5hXtV/VYmN6H5bCbBftcyq14+TE9YYtnRSTYkub619si8egOARTKXcB9uQPO2JFuSHNdau3sXq1+a5O4kr6iqI3b4jH2TvHV4e8E8+gKARTTzCXVVdVqSt2Ryx7lrkrymqnZebWtr7cIkaa19s6penUnIX1lVl2Ry+9kTM7lM7tJMbkkLAExhHmfLP3mY7pXkzGXWuSrJhdvftNY2V9ULkvx2Jren3TfJLUlem+SPd7wPPQCwMjOHe2vtnCTnTFF3XZKfmXV8+vEzPzPe7rB58+apax/3uMfNNPYf/MEfzFQ/iw0bNkxde8opp8w09r777jtT/RLfEO62rVu3zjT2H/7hH05d+/u///szjX3vvTtfgATfa0/ffhYAWGXCHQA6I9wBoDPCHQA6I9wBoDPCHQA6I9wBoDPCHQA6I9wBoDPCHQA6I9wBoDPCHQA6I9wBoDPCHQA6I9wBoDPVWhu7h7mrqi1JDh+7D1bmqquumrr2+c9//hw7YXc8/PDDM9V/7GMfm6n+Qx/60NS1F1988Uxjwyq6sbW2aaVFjtwBoDPCHQA6I9wBoDPCHQA6I9wBoDPCHQA6I9wBoDPCHQA6I9wBoDPCHQA6I9wBoDPCHQA6I9wBoDPCHQA6s/fYDcB2J5100tS173vf+0Ybe1Yf+chHpq797ne/O9PY55577tS1d9xxx0xjf/nLX56pHlieI3cA6IxwB4DOCHcA6IxwB4DOCHcA6IxwB4DOCHcA6IxwB4DOCHcA6IxwB4DOCHcA6IxwB4DOCHcA6IxwB4DOCHcA6Ey11sbuYe6qakuSw8fuAwBmdGNrbdNKixy5A0BnhDsAdEa4A0BnhDsAdEa4A0BnhDsAdEa4A0BnhDsAdEa4A0BnhDsAdEa4A0BnhDsAdEa4A0BnhDsAdEa4A0BnhDsAdGbmcK+qA6rq9Kr6cFXdUlUPVdX9VXVtVb2qqr5vp/UPqqq2i9cls/YEAIts7zl8xqlJLkhyR5Irktye5IlJfi7Je5O8qKpOba21ner+McnmJT7v83PoCQAW1jzC/eYkJyb5WGvtu9tnVtUbk/xtkpdlEvQf3Knus621c+YwPgCwg5m/lm+tXd5a+6sdg32Yf2eS9wxvj5l1HABg98zjyH1Xvj1MH11i2Y9U1a8kOSDJPUluaK3dtIf7AYDu7bFwr6q9k/zi8PbjS6zy08Nrx5ork5zWWrt9N8fYssyiQ3azTQDozp68FO5tSZ6Z5LLW2id2mL8tye8m2ZRk4/B6QSYn4x2T5FNVtd8e7AsAulbfexL7HD606jVJzk3yxSRHtdbu3Y2avZNcm+Q5Sc5srZ07w/hbkhw+bT0ArBE3ttY2rbRo7kfuVfXrmQT7PyU5dneCPUlaa49mculckhw9774AYFHMNdyr6swk52VyrfqxwxnzK/H1YepreQCY0tzCvap+K8k7knw2k2C/a4qPee4wvXVefQHAoplLuFfV2ZmcQLclyXGttbt3se7hO9+Sdph/XJKzhrfvn0dfALCIZr4UrqpOS/KWJN9Jck2S11TVzqttba1dOPz8R0meVlXXJ/naMO9ZSV44/Hx2a+36WfsCgEU1j+vcnzxM90py5jLrXJXkwuHni5K8NMlPJnlRku9P8q9J/jLJ+a21a+bQEwAsrD1yKdzYXAoHQCfWxqVwAMC4hDsAdEa4A0BnhDsAdEa4A0BnhDsAdEa4A0BnhDsAdEa4A0BnhDsAdEa4A0BnhDsAdEa4A0BnhDsAdEa4A0BnhDsAdEa4A0BnhDsAdEa4A0BnhDsAdEa4A0BnhDsAdEa4A0BnhDsAdEa4A0BnhDsAdEa4A0BnhDsAdEa4A0Bneg33g8ZuAADm4KBpivaecxNrxTeH6dZllh8yTL+451vphm02HdttOrbbytlm01nL2+2g/EeerUi11ubbyjpQVVuSpLW2aexe1gvbbDq223Rst5WzzabT63br9Wt5AFhYwh0AOiPcAaAzwh0AOiPcAaAzC3m2PAD0zJE7AHRGuANAZ4Q7AHRGuANAZ4Q7AHRGuANAZ4Q7AHRmocK9qp5UVX9WVf9SVY9U1daqemdVbRy7t7Vq2EZtmdedY/c3lqo6parOq6prquqbw/Z4/2PUHFlVl1XVvVX1UFXdVFVnVtVeq9X32Fay3arqoF3se62qLlnt/sdQVQdU1elV9eGqumXYd+6vqmur6lVVteS/44u+v610u/W2v/X6PPfvUVVPSXJ9kh9O8pFMnt37U0l+I8kJVXVUa+2eEVtcy+5P8s4l5j+wyn2sJW9K8uxMtsHX8h/PhF5SVZ2U5INJHk7yF0nuTfKzSd6R5Kgkp+7JZteQFW23wT8m2bzE/M/Pr6017dQkFyS5I8kVSW5P8sQkP5fkvUleVFWnth3uSGZ/SzLFdhv0sb+11hbileQTSVqS/7nT/D8a5r9n7B7X4ivJ1iRbx+5jrb2SHJvkaUkqyTHDPvT+ZdZ9fJK7kjyS5Igd5u+byS+cLckrxv5vWoPb7aBh+YVj9z3yNnthJsH8fTvNPzCTwGpJXrbDfPvbdNutq/1tIb6WH47aj88kqN610+L/neTBJL9QVfutcmusU621K1prX27DvwqP4ZQkP5Tkktba3+/wGQ9nciSbJL+2B9pcc1a43UjSWru8tfZXrbXv7jT/ziTvGd4es8Mi+1um2m5dWZSv5Y8dpp9c4n/ob1XVdZmE/3OTfGq1m1sH9qmqn0/yY5n8InRTkqtba98Zt61144XD9ONLLLs6ybYkR1bVPq21R1avrXXjR6rqV5IckOSeJDe01m4auae14tvD9NEd5tnfHttS2227Lva3RQn3ZwzTm5dZ/uVMwv3pEe5LOTDJRTvN+0pV/VJr7aoxGlpnlt3/WmuPVtVXkvx4koOTfGE1G1snfnp4/buqujLJaa2120fpaA2oqr2T/OLwdscgt7/twi6223Zd7G8L8bV8kv2H6f3LLN8+/wl7vpV1531Jjssk4PdL8hNJ/iSTv0/9dVU9e7zW1g3733S2JfndJJuSbBxeL8jk5Khjknxqwf+U9rYkz0xyWWvtEzvMt7/t2nLbrav9bVHCnSm11t48/O3qX1tr21prn2+t/WomJyL+pyTnjNshvWqt3dVa+53W2o2ttfuG19WZfMv2mSRPTXL6uF2Oo6pek+R1mVz18wsjt7Nu7Gq79ba/LUq4b/9Ndf9llm+ff9+eb6Ub209IOXrULtYH+98ctdYezeRSpmQB97+q+vUk5yb5pyTHttbu3WkV+9sSdmO7LWm97m+LEu5fGqZPX2b504bpcn+T53t9fZium6+pRrTs/jf8/e/JmZzYc+tqNrXOLeT+V1VnJjkvk2uujx3O/N6Z/W0nu7nddmXd7W+LEu5XDNPjl7gr0eMyuanDtiSfXu3G1rHnDtOF+QdiBpcP0xOWWHZ0kg1Jrl/gM5ensXD7X1X9ViY3oflsJgF11zKr2t92sILttivrbn9biHBvrf1zkk9mchLYGTstfnMmv41d1Fp7cJVbW9Oq6tClTiCpqoOSnD+83eUtV0mSXJrk7iSvqKojts+sqn2TvHV4e8EYja1lVXX4UrdWrarjkpw1vF2I/a+qzs7kRLAtSY5rrd29i9Xtb4OVbLfe9rdalHtJLHH72S8keU4m18DfnOTI5vaz/5+qOieTk0+uTnJbkm8leUqSF2dyt6vLkry0tfZvY/U4lqo6OcnJw9sDk/y3TH6rv2aYd3dr7fU7rX9pJrcDvSST24GemMllS5cm+e+LcGOXlWy34fKjp2Xy/9uvDcuflf+4jvvs1tr2sOpWVZ2W5MIk38nkq+WlzoLf2lq7cIeak7Pg+9tKt1t3+9vYt8hbzVeS/5LJpV13JPm3TALrnUk2jt3bWnxlchnIxZmcWXpfJjd++HqS/5vJdaI1do8jbptzMrlV5XKvrUvUHJXJL0TfSPJQks9lckSw19j/PWtxuyV5VZKPZnJnyQcyuZ3q7ZncK/35Y/+3rKFt1pJcaX+bbbv1tr8tzJE7ACyKhfibOwAsEuEOAJ0R7gDQGeEOAJ0R7gDQGeEOAJ0R7gDQGeEOAJ0R7gDQGeEOAJ0R7gDQGeEOAJ0R7gDQGeEOAJ0R7gDQGeEOAJ0R7gDQmf8HLnDFyEwA/YYAAAAASUVORK5CYII="
     },
     "metadata": {
      "image/png": {
       "width": 251,
       "height": 248
      },
      "needs_background": "light"
     }
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Inicialmente, podemos construir uma rede usando matrizes e multiplicação de matrizes. Depois, vamos refazer nossa rede usando as ferramentas do modulo `nn`. \n",
    "\n",
    "As camadas da rede MLP são chamadas de *fully-connected* ou *dense*, isso porque  todas as unidades de uma camada está conectada a todas as unidades da camada seguinte. As entradas de cada uma das camadas deve ser um vetor de uma única dimensão, e por isso nossas imagens de 28x28 devem ser convertidas em tensores de 784 unidades. Sendo assim, nosso tensor de tamanho `(64, 1, 28, 28)` é convertido para um de tamanho `(64, 784)`. Esse procedimento é chamado de *flattening*, nós achatamos um tensor de 2 dimensões em um tensor de 1 dimensão."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "## Definindo a função de ativação\n",
    "def sigmoid(x):\n",
    "    return 1/(1 + torch.exp(-x))\n",
    "\n",
    "# fazendo o flattening. Mantém o tamanho da primeira dimensão (64), referente ao tamanho do batch\n",
    "#    e transforma todas as outras dimensões em uma única dimensão.\n",
    "inputs =  images.view(images.shape[0],-1)\n",
    "\n",
    "# pesos conectando a camada de entrada (784) à camada escondida \n",
    "#    note que a camada escondida é composta por 256 neurônios\n",
    "W1 = torch.randn(inputs.shape[1] , 256)\n",
    "# pesos conectando a camada escondida à camada de saída.\n",
    "#    note que a camada de saída tem 10 neurônios pois queremos classificar 10 dígitos (classes)\n",
    "W2 = torch.randn(256, 10)\n",
    "\n",
    "# termos de bias para a camada escondida de camada de saída\n",
    "B1 = torch.randn(256)\n",
    "B2 = torch.randn(10)\n",
    "\n",
    "# computa os termos da camada escondida\n",
    "h = sigmoid(torch.mm(inputs,W1) + B1)\n",
    "\n",
    "# computa a saída da camada de saída, ou seja, a saída da rede\n",
    "out = torch.mm(h,W2) + B2"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Dadas essas 10 saídas, queremos apresentar uma imagem a rede e computar a probabilidade de pertencer a cada classe, o que, a princípio, será algo do tipo:\n",
    "\n",
    "\n",
    "<img src='assets/image_distribution.png' width=500px>\n",
    "\n",
    "A probabilidade para cada classe é mais ou menos a mesma, porque a rede não foi treinada.\n",
    "\n",
    "Para calcular essa distribuição de probabilidades, frequentemente é usada a função [**softmax**](https://en.wikipedia.org/wiki/Softmax_function), a qual pode ser definida como:\n",
    "\n",
    "$$\n",
    "\\Large \\sigma(x_i) = \\cfrac{e^{x_i}}{\\sum_k^K{e^{x_k}}}\n",
    "$$\n",
    "\n",
    "O que a função faz é `esmagar` a saida da rede para valores entre zero e um e depois normalizar esse valor, sendo assim, a soma das probabilidades de cada classe vai ser igual a 1."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "def softmax(x):\n",
    "    # dim = 1 é para executar a soma pelo eixo 1, percorrendo todas as 10 possíveis classes, \n",
    "    #    e não cada uma das amostras. A saída da softmax será um tensor de 64x10, ou seja, para cada\n",
    "    #    amostra, a probabilidade dela pertencer a cada uma das classes.\n",
    "    return torch.exp(x)/torch.sum(torch.exp(x),dim = 1).view(-1,1)\n",
    "\n",
    "# Aqui, probabilities recebe a saída da softmax, ou seja, o tensor com formato (64,10) \n",
    "probabilities = softmax(out)\n",
    "\n",
    "# verificando o formato (64, 10)\n",
    "print(probabilities.shape)\n",
    "# Verificando se a soma do valor das probabilidades de cada amostra é igual a 1.\n",
    "print(probabilities.sum(dim=1))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([64, 10])\n",
      "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Construíndo nossa rede usando PyTorch\n",
    "\n",
    "Agora vamos ver como fica a construção da nossa rede usando o modulo `nn` do PyTorch."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "from torch import nn\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Entradas para transformação linear da camada escondida\n",
    "        self.hidden = nn.Linear(784, 256)\n",
    "        # Camada de saída, 10 unidades, 1 para cada dígito\n",
    "        self.output = nn.Linear(256, 10)\n",
    "        \n",
    "        # Define a função Sigmoid e Softmax\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.softmax = nn.Softmax(dim=1) # dimensão 1 para passar por colunas\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Passa o tensor de entrada por cada uma das operações\n",
    "        x = self.hidden(x)\n",
    "        x = self.sigmoid(x)\n",
    "        x = self.output(x)\n",
    "        x = self.softmax(x)\n",
    "        \n",
    "        return x"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Indo por partes\n",
    "\n",
    "```python\n",
    "class Network(nn.Module):\n",
    "```\n",
    "\n",
    "Aqui herdamos da classe `nn.Module`. Combinada com `super().__init__()` criará uma classe que trilha a arquitetura e fornece varios atributos e métodos. Essa herança é obrigatória e qualquer nome pode ser dado à classe.\n",
    "\n",
    "```python\n",
    "self.hidden = nn.Linear(784, 256)\n",
    "```\n",
    "\n",
    "Essa linha cria um modulo para a transformação linear , $x\\mathbf{W} + b$, com 784 entradas e 256 saídas, e aqui chamada de `self.hidden`, para nossa camada escondida. O módulo cria automaticamente os tensores de pesos e bias, os quais serão usados no método `forward`. Esse pesos e bias podem ser acessados após instanciar a rede (`net`) usando os comandos `net.hidden.weight` e `net.hidden.bias`.\n",
    "\n",
    "```python\n",
    "self.output = nn.Linear(256, 10)\n",
    "```\n",
    "\n",
    "De forma similar, criamos outra transformação linear, com 256 entradas e 10 saídas.\n",
    "\n",
    "```python\n",
    "self.sigmoid = nn.Sigmoid()\n",
    "self.softmax = nn.Softmax(dim=1)\n",
    "```\n",
    "\n",
    "Aqui definimos a função de ativação Sigmoid para ativação, e a Softmax para computar as probabilidades. Setando `dim=1` na `nn.Softmax(dim=1)` estamos computando os valores para cada coluna.\n",
    "\n",
    "```python\n",
    "def forward(self, x):\n",
    "```\n",
    "\n",
    "As redes criadas usando o módulo `nn.Module` do PyTorch devem definir o método `forward`. Ela recebe como entrada um tensor `x` e passa ele pelas operações definidas no método `__init__`.\n",
    "\n",
    "```python\n",
    "x = self.hidden(x)\n",
    "x = self.sigmoid(x)\n",
    "x = self.output(x)\n",
    "x = self.softmax(x)\n",
    "```\n",
    "\n",
    "Aqui o tensor de entrada `x` é passado por cada uma das operações e o retorno é jogado de volta pra `x`. O vetor passa pela camada escondida, pela Sigmoid, pela camada de saída, e finalmente pela Softmax. A sequência em que esses métodos são definidos no método `__init__` não importa, mas eles devem ser definidos na ordem correta no método `forward`\n",
    "\n",
    "Agora podemos criar nosso objeto `Network`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# Criando a rede e visualizando sua representação em forma de texto.\n",
    "model = Network()\n",
    "model"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Network(\n",
       "  (hidden): Linear(in_features=784, out_features=256, bias=True)\n",
       "  (output): Linear(in_features=256, out_features=10, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       "  (softmax): Softmax(dim=1)\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "A mesma rede pode ser definida de modo mais consido e limpo usando o módulo `torch.nn.functional`. Para isso, importamos o módulo `F`, `import torch.nn.functional as F`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Entradas para transformação linear da camada escondida\n",
    "        self.hidden = nn.Linear(784, 256)\n",
    "        # Camada de saída, 10 unidades, 1 para cada dígito\n",
    "        self.output = nn.Linear(256, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Camada escondida com ativação Sigmoid\n",
    "        x = F.sigmoid(self.hidden(x))\n",
    "        # Camada escondida com ativação Softmax \n",
    "        x = F.softmax(self.output(x), dim=1)\n",
    "        \n",
    "        return x"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Funções de ativação\n",
    "\n",
    "Até agora nós utilizamos a função de ativação Sigmoid, mas no geral, qualquer função pode ser usada como uma função de ativação. O único requisito é que a função seja não linear. Aqui tem alguns exemplos de funções de ativação comuns: Tanh (Tangente hyperbolic), e ReLU (rectified linear unit).\n",
    "\n",
    "<img src=\"assets/activation.png\" width=700px>\n",
    "\n",
    "Na prática, ReLU é a função usada quase que exclusivamente para a ativação  de camadas escondidas."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Sua vez de construir uma rede neural\n",
    "\n",
    "<img src=\"assets/mlp_mnist.png\" width=600px>\n",
    "\n",
    "> **Exercício:** Crie uma rede neuraç com 784 neurônios de entrada, uma camada escondida de 128 unidades e função de ativação ReLU, outra camada escondida com 64 neurônios e novamente a função de ativação ReLU, e finalmente uma camada de saída com a Softmax como função de ativação, como mostrado na figura acima. Para usar a ReLU, pode usar o módulo `nn.ReLU` ou a função `F.relu`.\n",
    "\n",
    "Uma boa prática é nomear suas camadas por seu tipo, como 'fc', por exemplo, representando camadas _fully-connected_. Para varias camadas, use `fc1`, `fc2`, etc."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "## Coloque a sua solução aqui"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Passagem Forward \n",
    "\n",
    "Agora que temos nossa rede, vamos ver o que acontece quando apresentamos uma imagem a ela."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# Obtendo algumas imagens\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# Redimensionando essas imagens em vetores de 1 dimensão. \n",
    "#    Novo formato deve ser(tamanho do batch, número de canais de cor, pixels da imagem) \n",
    "images.resize_(64, 1, 784)\n",
    "# podemos usar tambem images.resize_(images.shape[0], 1, 784) para pegar o tamanho do batch automaticamente\n",
    "\n",
    "# Passagem Forward pela rede\n",
    "img_idx = 0\n",
    "ps = model.forward(images[img_idx,:])\n",
    "\n",
    "img = images[img_idx]\n",
    "helper.view_classify(img.view(1, 28, 28), ps)"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 432x648 with 2 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {
      "image/png": {
       "width": 424,
       "height": 235
      },
      "needs_background": "light"
     }
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Como pudemos ver, nossa rede não faz ideia de que digito seja. Isso porque ela foi inicializada com pesos aleatórios e ainda não foi treinada!\n",
    "\n",
    "\n",
    "### Usando `nn.Sequential`\n",
    "\n",
    "PyTorch também fornece um modo mais conveniente para construir redes mais simples, onde o tensor é passado de forma sequencial pelas operações, `nn.Sequential` ([documentação](https://pytorch.org/docs/master/nn.html#torch.nn.Sequential)). Podemos utilizá-lo para construir uma rede similar equivalente à acima:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "# Hyperparametros da rede \n",
    "input_size = 784\n",
    "hidden_sizes = [128, 64]\n",
    "output_size = 10\n",
    "\n",
    "# Construindo a rede feed-forward\n",
    "model = nn.Sequential(nn.Linear(input_size, hidden_sizes[0]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[1], output_size),\n",
    "                      nn.Softmax(dim=1))\n",
    "print(model)\n",
    "\n",
    "# Passo Forward de uma única amostra pela rede e mostrando a saída\n",
    "images, labels = next(iter(trainloader))\n",
    "images.resize_(images.shape[0], 1, 784)\n",
    "ps = model.forward(images[0,:])\n",
    "helper.view_classify(images[0].view(1, 28, 28), ps)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=784, out_features=128, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=64, out_features=10, bias=True)\n",
      "  (5): Softmax(dim=1)\n",
      ")\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 432x648 with 2 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {
      "image/png": {
       "width": 424,
       "height": 235
      },
      "needs_background": "light"
     }
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Nosso modelo aqui é o mesmo que o de antes, com 784 unidades de entrada, uma camada oculta com 128 unidades, ativação ReLU, camada oculta com 64 unidades seguida por outra ReLU, e então a camada de saída com 10 unidades seguida pela Softmax.\n",
    "\n",
    "As operações ficam disponíveis se consultadas pelo indice apropriado. Podemos, por exemplo, consultar os pesos da primeira operação linear usando `model[0]`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "print(model[0])\n",
    "print(model[0].weight)\n",
    "#print(model[0].bias)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Linear(in_features=784, out_features=128, bias=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0013, -0.0092, -0.0097,  ...,  0.0190, -0.0248, -0.0249],\n",
      "        [-0.0031, -0.0006,  0.0113,  ...,  0.0125,  0.0296, -0.0122],\n",
      "        [ 0.0039,  0.0252,  0.0019,  ..., -0.0185, -0.0200, -0.0295],\n",
      "        ...,\n",
      "        [ 0.0209,  0.0160,  0.0244,  ..., -0.0158, -0.0334, -0.0072],\n",
      "        [ 0.0331, -0.0016, -0.0300,  ...,  0.0100, -0.0347, -0.0072],\n",
      "        [ 0.0103,  0.0169,  0.0038,  ..., -0.0170,  0.0310,  0.0012]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Podemos também passar um `OrderedDict` para dar um nome a cada camada de forma individual, e assim buscar por elas sem precisar passar um índice. Note que as chaves desse dicionário devem ser únicas, então _cada operação deve ter um nome diferente_."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "from collections import OrderedDict\n",
    "model = nn.Sequential(OrderedDict([\n",
    "                      ('fc1', nn.Linear(input_size, hidden_sizes[0])),\n",
    "                      ('relu1', nn.ReLU()),\n",
    "                      ('fc2', nn.Linear(hidden_sizes[0], hidden_sizes[1])),\n",
    "                      ('relu2', nn.ReLU()),\n",
    "                      ('output', nn.Linear(hidden_sizes[1], output_size)),\n",
    "                      ('softmax', nn.Softmax(dim=1))]))\n",
    "model"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (fc1): Linear(in_features=784, out_features=128, bias=True)\n",
       "  (relu1): ReLU()\n",
       "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (relu2): ReLU()\n",
       "  (output): Linear(in_features=64, out_features=10, bias=True)\n",
       "  (softmax): Softmax(dim=1)\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Agora podemos acessar as camadas tanto pelo indice quanto pelo nome."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "print(model[0])\n",
    "print(model.fc1)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Linear(in_features=784, out_features=128, bias=True)\n",
      "Linear(in_features=784, out_features=128, bias=True)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Treinando a rede Neural\n",
    "\n",
    "A rede que construimos ainda não é tão esperta, e não é capaz de reconhecer os dígitos:\n",
    "\n",
    "<img src=\"assets/function_approx.png\" width=500px>\n",
    "\n",
    "A princípio, a função responsável por mapear as entradas para as saídas é composta por pesos aleatórios. Nós vamos treinar o modelo apresentando dados reais, e ajustando esses pesos a fim de aproximar a saída da função do rótulo real esperado.\n",
    "\n",
    "Para encontrar esses pesos, ou parametros, precisamos identificar o quanto a rede está errando, e pra isso usamos a já conhecida **funçao loss** (também chamada de função de custo), uma medida de quanto as estimativas da rede estão errando. Uma função loss que já vimos para regressão e classificação é o erro médio quadrático:\n",
    "\n",
    "$$\n",
    "\\large \\ell = \\frac{1}{2n}\\sum_i^n{\\left(y_i - \\hat{y}_i\\right)^2}\n",
    "$$\n",
    "onde $n$ é o número de amostras de treinamento, $y_i$ são os rótulos verdadeiros, e $\\hat{y}_i$ são os rótulos estimados.\n",
    "\n",
    "Minimizando esse erro ao ajustar os pesos da rede conseguimos encontrar a configuração em que o erro é mínimo e a rede fica pronta para estimar os rótulos que fornecem a melhor acurácia. Podemos encontrar esse erro mínimo utilizando um processo chamado **gradiente descendente**. O gradiente é o vetor de derivadas, ou seja, inclinação da função de custo em direção ao mínimo da função. Para alcançar o ponto mínimo, devemos seguir o gradiente na direção de sua descida. Pense se estivesse descendo uma montanha e cada iteração fosse um passo na direção de sua base.\n",
    "\n",
    "<img src='assets/gradient_descent.png' width=350px>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Backpropagation\n",
    "\n",
    "Para redes com uma única camada, o gradiente descendente é mais fácil de implementar, como vimos na última aula. No entanto, fica mais complicapo para redes mais profundas, como a que construímos. A complexidade é tanta que levou cerca de 30 anos até que os pesquisadores descobrissem uma forma adequada de treinar essas redes.\n",
    "\n",
    "O treinamento de redes multi-camadas é feito pelo algoritmo **backpropagation**, o qual é apenas uma aplicação da regra de cadeia em calculo. É mais fácil entender se convertermos uma rede de 2 camadas em uma representação gráfica.\n",
    "\n",
    "\n",
    "\n",
    "<img src='assets/backprop_diagram.png' width=550px>\n",
    "\n",
    "No passo forward, os dados e operações seguem de baixo pra cima. A entrada $x$ passa pela transformação linear\n",
    "$L_1$ com pesos $W_1$ e biases $b_1$. A saída passa pela Sigmoid $S$ e outra camada linear $L_2$. Finalmente computamos a saída da função loss $\\ell$, o qual é usado para medir o quanto a rede está errando. O objetivo é ajustar os pesos e bias para minimizar $\\ell$.\n",
    "\n",
    "Para treinar os pesos com gradiente descendente, propagamos o gradiente do erro de volta pela rede. Cada operação tem alguns gradientes entre a entrada e a saída. Conforme enviamos os gradientes de volta, multiplicamos o gradiente atual pelo gradiente da operação. Matematicamente falando, o que o método faz é apenas calcular o gradiente do erro em relação aos pesos usando a regra da cadeia.\n",
    "\n",
    "\n",
    "$$\n",
    "\\large \\frac{\\partial \\ell}{\\partial W_1} = \\frac{\\partial L_1}{\\partial W_1} \\frac{\\partial S}{\\partial L_1} \\frac{\\partial L_2}{\\partial S} \\frac{\\partial \\ell}{\\partial L_2}\n",
    "$$\n",
    "\n",
    "Os pesos são atualizados usando esse gradiente em conjunto com uma taxa de aprendizagem $\\alpha$. \n",
    "\n",
    "$$\n",
    "\\large W^\\prime_1 = W_1 - \\alpha \\frac{\\partial \\ell}{\\partial W_1}\n",
    "$$\n",
    "\n",
    "A taxa de aprendizagem $\\alpha$ é escolhida de forma que a atualização dos pesos a cada passo seja pequena o suficiente para que o método chegue ao valor mínimo de loss de forma iterativa."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Funções de loss em PyTorch\n",
    "\n",
    "Vamos relembrar como calcular funções de loss usando PyTorch. Usando o módulo `nn`, podemos encontrar várias funções _loss_, como por exemplo a entropia cruzada (`nn.CrossEntropyLoss`). Essa função geralmente é atribuída como `criterion`. Como vimos anteriormente, classificação multi-classe, como no caso da MNIST, usamos a softmax para predizer a probabilidade de cada classe. Com a saída da softmax, queremos usar a entropia cruzada como função de _loss_. Para calcular o erro de fato, precisamos definir o critério e passar os rótulos corretos à nossa rede.\n",
    "\n",
    "\n",
    "\n",
    "Note um ponto muito importante na documentação:[documentação de `nn.CrossEntropyLoss`](https://pytorch.org/docs/stable/nn.html#torch.nn.CrossEntropyLoss),\n",
    "\n",
    "> Esse critério combina `nn.LogSoftmax()` e `nn.NLLLoss()` em uma única classe.\n",
    ">\n",
    "> A saída deve conter a pontuação esperada para cada classe.\n",
    "\n",
    "Isso significa que precisamos passar diretamente a saída da rede para computar o _loss_, em vez de fornecer a saída após passar pela função Softmax. Essa saída direta (antes da Softmax) é chamada *logit* ou *scores*. Usamos os logits porque as probabilidades fornecidas pela Softmax ficam muito próximas de 0 ou 1. ([leia mais aqui](https://docs.python.org/3/tutorial/floatingpoint.html)). Ou seja, é melhor evitar calculos usando probabilidades, uma vez que é mais comum usar o log das probabilidades."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Define as transformações para normalizar os dados\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5,), (0.5,)),\n",
    "                              ])\n",
    "# Baixa e/ou carrega os dados\n",
    "trainset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "# Construindo uma rede feed-forward\n",
    "model = nn.Sequential(nn.Linear(784, 128),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(128, 64),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(64, 10))\n",
    "\n",
    "# Definindo a função loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Obtendo os dados\n",
    "images, labels = next(iter(trainloader))\n",
    "# achatando a imagem (2-D para 1-D)\n",
    "images = images.view(images.shape[0], -1)\n",
    "\n",
    "# Passagem Forward pass, pegando os logits\n",
    "logits = model(images)\n",
    "# Calculando o erro com os logits e os rótulos\n",
    "loss = criterion(logits, labels)\n",
    "\n",
    "print(loss)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor(2.3219, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "As vezes pode ser mais conveniente construir um modelo usando como saída a função log-softmax usando `nn.LogSoftmax` ou `F.log_softmax` ([documentação](https://pytorch.org/docs/stable/nn.html#torch.nn.LogSoftmax)). Então você pode obter as probabilidades reais através da exponencial `torch.exp(output)`. Com a saída da log-softmax, podemos computar o _negative log likelihood_, `nn.NLLLoss` ([documentação](https://pytorch.org/docs/stable/nn.html#torch.nn.NLLLoss)), o que é equivalente à saída da `nn.CrossEntropyLoss`. \n",
    "\n",
    ">**Exercício:** Construa um modelo que retorne o log-softmax como saída e calcule a função de loss utilizando o _negative log likelihood_. Note que para `nn.LogSoftmax` e `F.log_softmax` você precisa setar o parâmetro `dim` de forma apropriada. `dim=0` computa o softmax pelas linhas (amostras), de forma que as colunas somem 1, enquanto `dim=1` calcula a softmax pelas colunas, de forma que as linhas somem 1. Pense no que deseja como saída e escolha `dim` de forma apropriada.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# TODO: Construa sua rede feed-forward com saída log-softmax\n",
    "model = \n",
    "\n",
    "# TODO: Defina a função de loss\n",
    "criterion = \n",
    "\n",
    "### Rode o trecho para testar se funcionou\n",
    "# Obtém os dados\n",
    "images, labels = next(iter(trainloader))\n",
    "# achatando a imagem (2-D para 1-D)\n",
    "images = images.view(images.shape[0], -1)\n",
    "\n",
    "# Passagem Forward pass, pegando os logits\n",
    "logps = model(images)\n",
    "# Calculando o erro com os logits e os rótulos\n",
    "loss = criterion(logps, labels)\n",
    "\n",
    "print(loss)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Autograd\n",
    "\n",
    "Agora que sabemos como computar o _loss_, como usá-lo no backpropagation? O PyTorch tem um módulo chamado `autograd` que computa os gradientes de forma automática. Autograd armazena as operações executadas em cada tensor, então faz o passe de volta calculando os gradientes pelo caminho. Para ter certeza que PyTorch está armazenando as operações de um tensor, devemos setar `requires_grad = True` no tensor. Podemos fazer isso em sua criação com a chamada `requires_grad`, ou a qualquer momente com `x.requires_grad_(True)`.\n",
    "\n",
    "Também é possível desligar os gradiente de um bloco usando `torch.no_grad()`:\n",
    "\n",
    "```python\n",
    "x = torch.zeros(1, requires_grad=True)\n",
    ">>> with torch.no_grad():\n",
    "...     y = x * 2\n",
    ">>> y.requires_grad\n",
    "False\n",
    "```\n",
    "\n",
    "Além disso, podemos ligar ou desligar a armazenagem de todos os gradientes ao mesmo tempo usando `torch.set_grad_enabled(True|False)`.\n",
    "\n",
    "Os gradiente são computados em relação a alguma variável `z` com `z.backward()`. Isso faz o passo de volta pela operação que criou `z`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "x = torch.randn(2,2, requires_grad=True)\n",
    "print(x)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[-0.6396,  0.4076],\n",
      "        [ 0.8627, -0.0147]], requires_grad=True)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "y = x**2\n",
    "print(y)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[4.0910e-01, 1.6612e-01],\n",
      "        [7.4429e-01, 2.1601e-04]], grad_fn=<PowBackward0>)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Abaixo podemos ver a operação que criou `y`, uma operação ao quadrado `PowBackward0`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "## grad_fn mostra a função que gerou essa variável\n",
    "print(y.grad_fn)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<PowBackward0 object at 0x7fc3fc6c4f40>\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "O modulo autograd mantém registro dessas operações e sabe como computar o gradiente de cada uma. Dessa forma, ele consegue computar o gradiente para uma cadeia de operações, com relaçao a qualquer tensor. Vamos reduzir o tensor `y` para um escalar, computando a média."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "z = y.mean()\n",
    "print(z)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor(0.3299, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Podemos verificar os gradientes de `x` e `y`, mas no momento eles estão vazios."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "print(x.grad)\n",
    "print(y.grad)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "None\n",
      "None\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/tmp/ipykernel_4813/106071707.py:2: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
      "  print(y.grad)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Para computar os gradientes, precisamos executar o comando `.backward` na variável `z`, por exemplo. isso irá computar o gradiente de `z` em relação a `x`.\n",
    "\n",
    "$$\n",
    "\\frac{\\partial z}{\\partial x} = \\frac{\\partial}{\\partial x}\\left[\\frac{1}{n}\\sum_i^n x_i^2\\right] = \\frac{x}{2}\n",
    "$$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "z.backward()\n",
    "print(x.grad)\n",
    "print(x/2)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[-0.3198,  0.2038],\n",
      "        [ 0.4314, -0.0073]])\n",
      "tensor([[-0.3198,  0.2038],\n",
      "        [ 0.4314, -0.0073]], grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "O calculo dos gradientes é muito importante para qualquer rede neural. Uma vez que conhecemos o gradiente, podemos executar o gradiente descendente."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Juntando a função Loss e Autograd \n",
    "\n",
    "Quando criamos uma rede com PyTorch, todos os parametros são inicializados com `requires_grad = True`. Isso significa que quando computamos o erro e chamamos `loss.backward()`, os gradientes dos parâmetros são computados. Esses gradientes são usados para atualizar os pesos usando gradiente descendente. Abaixo vemos um exemplo de como computar os gradientes usando a passagem de volta (_backward pass_)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "# Construindo uma rede feed-forward\n",
    "model = nn.Sequential(nn.Linear(784, 128),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(128, 64),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(64, 10),\n",
    "                      nn.LogSoftmax(dim=1))\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "images, labels = next(iter(trainloader))\n",
    "images = images.view(images.shape[0], -1)\n",
    "\n",
    "logits = model(images)\n",
    "loss = criterion(logits, labels)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "print('Before backward pass: \\n', model[0].weight.grad)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print('After backward pass: \\n', model[0].weight.grad)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Before backward pass: \n",
      " None\n",
      "After backward pass: \n",
      " tensor([[ 0.0056,  0.0056,  0.0056,  ...,  0.0056,  0.0056,  0.0056],\n",
      "        [ 0.0030,  0.0030,  0.0030,  ...,  0.0030,  0.0030,  0.0030],\n",
      "        [ 0.0013,  0.0013,  0.0013,  ...,  0.0013,  0.0013,  0.0013],\n",
      "        ...,\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0004, -0.0004, -0.0004,  ..., -0.0004, -0.0004, -0.0004],\n",
      "        [ 0.0017,  0.0017,  0.0017,  ...,  0.0017,  0.0017,  0.0017]])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Treinando a rede!\n",
    "\n",
    "A última peça que precisamos para começar a treinar é a seleção do otimizador, o qual utilizaremos para atualizar os pesos com o gradiente. Esses otimizadores estão em disponíveis em [`optim` package](https://pytorch.org/docs/stable/optim.html). Como exemplo, podemos usar o gradiente descendente estocástico (SGD) com `optim.SGD`. Definimos da seguinte forma:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "from torch import optim\n",
    "\n",
    "# Optimizers require the parameters to optimize and a learning rate\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Agora que sabemos como utilizar as partes individuais, é hora de juntar tudo. O processo geral pode ser definido da seguinte forma:\n",
    "\n",
    "* Fazer a passagem forward na rede\n",
    "* Usar a saída da rede para computar o o erro (_loss_)\n",
    "* Fazer a passagem de volta (backward) pela rede com `loss.backward()` para computar os gradientes\n",
    "* Utilizar o otimizador para atualizar os pesos\n",
    "\n",
    "Abaixo passamos por um passo do treinamento e printamos como os pasos e gradientes vão mudando. Note que a linha `optimizer.zero_grad()` é usada para zerar os gradientes, que são acumulados a cada bassagem backward. Ou seja, esses valores devem ser zerados a cada iteração para evitar os valores acumulados em outras passadas."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "print('Pesos iniciais - ', model[0].weight)\n",
    "\n",
    "images, labels = next(iter(trainloader))\n",
    "images.resize_(64, 784)\n",
    "\n",
    "# Limpa os gradientes acumulados\n",
    "optimizer.zero_grad()\n",
    "\n",
    "# Passagens Forward, backward, e atualização dos pesos\n",
    "output = model(images)\n",
    "loss = criterion(output, labels)\n",
    "loss.backward()\n",
    "print('Gradiente -', model[0].weight.grad)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Pesos iniciais -  Parameter containing:\n",
      "tensor([[ 0.0118,  0.0069,  0.0196,  ..., -0.0076, -0.0353,  0.0085],\n",
      "        [-0.0042,  0.0311,  0.0242,  ...,  0.0020, -0.0070,  0.0026],\n",
      "        [ 0.0263,  0.0107, -0.0345,  ...,  0.0307,  0.0294, -0.0100],\n",
      "        ...,\n",
      "        [ 0.0064,  0.0209,  0.0116,  ..., -0.0026,  0.0081,  0.0276],\n",
      "        [-0.0341,  0.0036,  0.0144,  ..., -0.0142,  0.0072,  0.0209],\n",
      "        [-0.0197, -0.0043, -0.0247,  ..., -0.0097, -0.0247,  0.0042]],\n",
      "       requires_grad=True)\n",
      "Gradiente - tensor([[-0.0038, -0.0038, -0.0038,  ..., -0.0038, -0.0038, -0.0038],\n",
      "        [ 0.0009,  0.0009,  0.0009,  ...,  0.0009,  0.0009,  0.0009],\n",
      "        [-0.0018, -0.0018, -0.0018,  ..., -0.0018, -0.0018, -0.0018],\n",
      "        ...,\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0005,  0.0005,  0.0005,  ...,  0.0005,  0.0005,  0.0005],\n",
      "        [ 0.0005,  0.0005,  0.0005,  ...,  0.0005,  0.0005,  0.0005]])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "# Passo de atualização dos pesos\n",
    "optimizer.step()\n",
    "print('Pesos Atualizados - ', model[0].weight)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Pesos Atualizados -  Parameter containing:\n",
      "tensor([[ 0.0118,  0.0070,  0.0197,  ..., -0.0076, -0.0353,  0.0085],\n",
      "        [-0.0042,  0.0311,  0.0242,  ...,  0.0020, -0.0070,  0.0026],\n",
      "        [ 0.0263,  0.0108, -0.0344,  ...,  0.0307,  0.0294, -0.0100],\n",
      "        ...,\n",
      "        [ 0.0064,  0.0209,  0.0116,  ..., -0.0026,  0.0081,  0.0276],\n",
      "        [-0.0341,  0.0036,  0.0144,  ..., -0.0142,  0.0072,  0.0208],\n",
      "        [-0.0197, -0.0043, -0.0247,  ..., -0.0097, -0.0247,  0.0042]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Treinando de verdade\n",
    "\n",
    "Agora vamos colocar o algoritmo dentro de um laço para que possamos iterar por todas as imagens. Uma passagem por todas as amostras de treinamento é chamada época. Vamos iterar pelo `trainloader` para obter nossos batches de treinamento. Para cada batch, vamos fazer uma passada de treinamento onde é computado o erro, faz o backpropagation e atualiza os pesos.\n",
    "\n",
    ">**Exercicio:** Implemente o treinamento completo de nossa rede. Se for implementada corretamente, você deverá ver o _loss_ diminuindo a cada época."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "## Sua solução vai aqui\n",
    "\n",
    "# TODO: defina o modelo, o critério e o atualizador\n",
    "\n",
    "model = nn.Sequential(nn.Linear(784, 128), \n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(128, 64),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(64, 10),\n",
    "                      nn.LogSoftmax(dim=1))\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.003)\n",
    "                      \n",
    "\n",
    "epochs = 5\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    \n",
    "    for images, labels in trainloader:\n",
    "        # Flatten MNIST images into a 784 long vector\n",
    "        images = images.view(images.shape[0], -1)\n",
    "    \n",
    "        # TODO: implemente a etapa de treinamento\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(images)\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    else:\n",
    "        print(f\"Training loss: {running_loss/len(trainloader)}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training loss: 1.8433859735282498\n",
      "Training loss: 0.7980094995897716\n",
      "Training loss: 0.5086987505176428\n",
      "Training loss: 0.42494959010879624\n",
      "Training loss: 0.3849027123945608\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Com a rede treinada, podemos verificar as predições estimadas."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "source": [
    "%matplotlib inline\n",
    "import helper\n",
    "\n",
    "images, labels = next(iter(trainloader))\n",
    "\n",
    "img = images[0].view(1, 784)\n",
    "# desligando o gradiente para acelerar o processo\n",
    "with torch.no_grad():\n",
    "    logps = model(img)\n",
    "\n",
    "# As saidas da rede são o log da probabilidade, por isso precisamos fazer o exponencial para termos as \n",
    "#    probabilidades reais.\n",
    "ps = torch.exp(logps)\n",
    "helper.view_classify(img.view(1, 28, 28), ps)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f8b48a08561fd7e728b99163935dfd430a6e9dd7380afe98d8e7cbd9767295b8"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.11 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}